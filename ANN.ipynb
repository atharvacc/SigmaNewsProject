{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataFiles():\n",
    "    market_df = pickle.load(open('Market_train',\"rb\"))\n",
    "    news_df = pickle.load(open(\"News_train\", \"rb\"))\n",
    "    print('Finished loading datafiles!')\n",
    "    return market_df, news_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(mkt_df, news_df):\n",
    "    mkt_df['time'] = pd.to_datetime(mkt_df['time'])\n",
    "    news_df['time'] = pd.to_datetime(news_df['time'])\n",
    "    mkt_df['time'] = mkt_df['time'].dt.date\n",
    "    news_df['time'] = news_df['time'].dt.date\n",
    "    assetCodes = []\n",
    "    index = 0\n",
    "    for x in news_df['assetCodes']:\n",
    "        x = x.split(',')[0].split(\"'\")[1]\n",
    "        assetCodes.append(x)\n",
    "    news_df['assetCode'] = np.asarray(assetCodes)\n",
    "    irrelevantColumns = ['sourceTimestamp', 'firstCreated', 'sourceId', \n",
    "                         'headline', 'provider', 'subjects', 'audiences',\n",
    "                        'headlineTag', 'marketCommentary', 'assetCodes', 'assetName']\n",
    "    news_df.drop(irrelevantColumns, axis=1, inplace=True)\n",
    "    mkt_df.drop(['assetName'], axis=1, inplace=True)\n",
    "    modifiednews = news_df.groupby(['time','assetCode'], sort=False).aggregate(np.mean).reset_index()\n",
    "    \n",
    "    # join news reports to market data, note many assets will have many days without news data\n",
    "    merged = pd.merge(mkt_df, modifiednews, how='left', on=['time', 'assetCode'], copy=False) \n",
    "    merged = merged.fillna(0)\n",
    "    print('Finished preprocessing data!')\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading datafiles!\n"
     ]
    }
   ],
   "source": [
    "market_data, news_data = loadDataFiles()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preprocessing data!\n"
     ]
    }
   ],
   "source": [
    "X = preprocess_data(market_data, news_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeY(ydf):\n",
    "    ydf = (ydf + 1) / 2\n",
    "    return ydf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[X['returnsOpenNextMktres10'] >= -1]\n",
    "X = X[X['returnsOpenNextMktres10'] <= 1]\n",
    "\n",
    "y = X['returnsOpenNextMktres10']\n",
    "\n",
    "X.drop(['returnsOpenNextMktres10'], axis=1, inplace=True)\n",
    "y = normalizeY(y)\n",
    "assetCodesAndTime = X.iloc[:, :2]\n",
    "X = X.iloc[:, 2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNNModel(numhiddenlayers=2, nodes=4): # returns NN given hidden layers and nodes\n",
    "    layers = []\n",
    "    layers.append(keras.layers.Flatten(input_shape=(35,)))\n",
    "\n",
    "    for x in range(numhiddenlayers):\n",
    "        layers.append(keras.layers.Dense(nodes, activation=tf.nn.relu, use_bias=True))\n",
    "\n",
    "    layers.append(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
    "    model = keras.Sequential(layers)\n",
    "    sgd = keras.optimizers.SGD(lr=.3)\n",
    "    model.compile(optimizer=sgd,\n",
    "              loss='mean_absolute_error',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinearRegressionModel(numfeatures):\n",
    "    inputs = keras.layers.Input(shape=(numfeatures,))\n",
    "    preds = keras.layers.Dense(1,activation='linear')(inputs)\n",
    "    model = keras.Model(inputs=inputs,outputs=preds)\n",
    "    sgd=keras.optimizers.SGD()\n",
    "    model.compile(optimizer=sgd ,loss='mse',metrics=['mse'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(df):\n",
    "    for column in df:\n",
    "        colmin = np.amin(df[column])\n",
    "        colmax = np.amax(df[column])\n",
    "        df[column] = (df[column] - colmin) / (colmax - colmin)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(X, y, split):\n",
    "    index = int(split*len(y.index))\n",
    "    y_train, y_test = np.split(y, [index])\n",
    "    X_train, X_test = X.iloc[:index, :], X.iloc[index:, :]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = regularize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, model_name):\n",
    "    model.save(model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(filename):\n",
    "    model = load_model(filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1221660 samples, validate on 2850543 samples\n",
      "Epoch 1/1\n",
      "1221660/1221660 [==============================] - 6s 5us/step - loss: 0.0293 - acc: 0.0000e+00 - val_loss: 0.0216 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x137c46d68>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnmodel = getNNModel(3, 15)\n",
    "nnmodel.fit(X, y, epochs=1, verbose=1, batch_size=100000, validation_split=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossMatrix(X, y):\n",
    "    layers = [3, 4, 5]\n",
    "    nodes = [10, 15, 20]\n",
    "    lossmatrix = []\n",
    "    X_train, y_train, X_test, y_test = splitDataset(X, y, .7)\n",
    "    for layer in layers:\n",
    "        lossforlayer = []\n",
    "        for node in nodes:\n",
    "            nnmodel2 = getNNModel(layer, node)\n",
    "            nnmodel2.fit(X_train , y_train, epochs=1, verbose=1, batch_size=1000000)\n",
    "            loss, acc = nnmodel2.evaluate(X_test, y_test)\n",
    "            lossforlayer.append(loss)\n",
    "        lossmatrix.append(lossforlayer)\n",
    "    for x in lossmatrix:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4072203\n",
      "Epoch 1/1\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.0322 - acc: 0.0000e+00\n",
      "1221661/1221661 [==============================] - 25s 21us/step\n",
      "Epoch 1/1\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.0377 - acc: 0.0000e+00\n",
      "1221661/1221661 [==============================] - 26s 21us/step\n",
      "Epoch 1/1\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0251 - acc: 0.0000e+00\n",
      "1221661/1221661 [==============================] - 26s 22us/step\n",
      "Epoch 1/1\n",
      "2850542/2850542 [==============================] - 7s 3us/step - loss: 0.0245 - acc: 0.0000e+00\n",
      "1221661/1221661 [==============================] - 29s 24us/step\n",
      "Epoch 1/1\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0244 - acc: 0.0000e+00\n",
      "1221661/1221661 [==============================] - 27s 22us/step\n",
      "Epoch 1/1\n",
      "2850542/2850542 [==============================] - 7s 3us/step - loss: 0.0246 - acc: 0.0000e+00\n",
      "1221661/1221661 [==============================] - 27s 22us/step\n",
      "Epoch 1/1\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0240 - acc: 0.0000e+00\n",
      "1221661/1221661 [==============================] - 30s 24us/step\n",
      "Epoch 1/1\n",
      "2850542/2850542 [==============================] - 8s 3us/step - loss: 0.0236 - acc: 0.0000e+00\n",
      "1221661/1221661 [==============================] - 33s 27us/step\n",
      "Epoch 1/1\n",
      "2850542/2850542 [==============================] - 10s 4us/step - loss: 0.0242 - acc: 0.0000e+00\n",
      "1221661/1221661 [==============================] - 32s 26us/step\n",
      "[0.02924319398984977, 0.02407273787866651, 0.02244628967740677]\n",
      "[0.022060430283365993, 0.02228930966185683, 0.021880768763167115]\n",
      "[0.021853938172650998, 0.021630632141529542, 0.02181874975266783]\n"
     ]
    }
   ],
   "source": [
    "# lossMatrix(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2850542 samples, validate on 1221661 samples\n",
      "Epoch 1/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.4866 - mean_squared_error: 0.4866 - val_loss: 0.4279 - val_mean_squared_error: 0.4279\n",
      "Epoch 2/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.3870 - mean_squared_error: 0.3870 - val_loss: 0.3425 - val_mean_squared_error: 0.3425\n",
      "Epoch 3/30\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.3084 - mean_squared_error: 0.3084 - val_loss: 0.2749 - val_mean_squared_error: 0.2749\n",
      "Epoch 4/30\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.2464 - mean_squared_error: 0.2464 - val_loss: 0.2214 - val_mean_squared_error: 0.2214\n",
      "Epoch 5/30\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.1974 - mean_squared_error: 0.1974 - val_loss: 0.1790 - val_mean_squared_error: 0.1790\n",
      "Epoch 6/30\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.1587 - mean_squared_error: 0.1587 - val_loss: 0.1455 - val_mean_squared_error: 0.1455\n",
      "Epoch 7/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.1281 - mean_squared_error: 0.1281 - val_loss: 0.1189 - val_mean_squared_error: 0.1189\n",
      "Epoch 8/30\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.1039 - mean_squared_error: 0.1039 - val_loss: 0.0977 - val_mean_squared_error: 0.0977\n",
      "Epoch 9/30\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.0847 - mean_squared_error: 0.0847 - val_loss: 0.0809 - val_mean_squared_error: 0.0809\n",
      "Epoch 10/30\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.0695 - mean_squared_error: 0.0695 - val_loss: 0.0675 - val_mean_squared_error: 0.0675\n",
      "Epoch 11/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0575 - mean_squared_error: 0.0575 - val_loss: 0.0569 - val_mean_squared_error: 0.0569\n",
      "Epoch 12/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0479 - mean_squared_error: 0.0479 - val_loss: 0.0483 - val_mean_squared_error: 0.0483\n",
      "Epoch 13/30\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.0403 - mean_squared_error: 0.0403 - val_loss: 0.0415 - val_mean_squared_error: 0.0415\n",
      "Epoch 14/30\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.0342 - mean_squared_error: 0.0342 - val_loss: 0.0360 - val_mean_squared_error: 0.0360\n",
      "Epoch 15/30\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.0294 - mean_squared_error: 0.0294 - val_loss: 0.0316 - val_mean_squared_error: 0.0316\n",
      "Epoch 16/30\n",
      "2850542/2850542 [==============================] - 6s 2us/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0280 - val_mean_squared_error: 0.0280\n",
      "Epoch 17/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0251 - val_mean_squared_error: 0.0251\n",
      "Epoch 18/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0227 - val_mean_squared_error: 0.0227\n",
      "Epoch 19/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0208 - val_mean_squared_error: 0.0208\n",
      "Epoch 20/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0192 - val_mean_squared_error: 0.0192\n",
      "Epoch 21/30\n",
      "2850542/2850542 [==============================] - 8s 3us/step - loss: 0.0148 - mean_squared_error: 0.0148 - val_loss: 0.0178 - val_mean_squared_error: 0.0178\n",
      "Epoch 22/30\n",
      "2850542/2850542 [==============================] - 9s 3us/step - loss: 0.0137 - mean_squared_error: 0.0137 - val_loss: 0.0167 - val_mean_squared_error: 0.0167\n",
      "Epoch 23/30\n",
      "2850542/2850542 [==============================] - 8s 3us/step - loss: 0.0128 - mean_squared_error: 0.0128 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n",
      "Epoch 24/30\n",
      "2850542/2850542 [==============================] - 9s 3us/step - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.0149 - val_mean_squared_error: 0.0149\n",
      "Epoch 25/30\n",
      "2850542/2850542 [==============================] - 8s 3us/step - loss: 0.0113 - mean_squared_error: 0.0113 - val_loss: 0.0142 - val_mean_squared_error: 0.0142\n",
      "Epoch 26/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0107 - mean_squared_error: 0.0107 - val_loss: 0.0136 - val_mean_squared_error: 0.0136\n",
      "Epoch 27/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0102 - mean_squared_error: 0.0102 - val_loss: 0.0131 - val_mean_squared_error: 0.0131\n",
      "Epoch 28/30\n",
      "2850542/2850542 [==============================] - 7s 3us/step - loss: 0.0098 - mean_squared_error: 0.0098 - val_loss: 0.0126 - val_mean_squared_error: 0.0126\n",
      "Epoch 29/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0094 - mean_squared_error: 0.0094 - val_loss: 0.0122 - val_mean_squared_error: 0.0122\n",
      "Epoch 30/30\n",
      "2850542/2850542 [==============================] - 7s 2us/step - loss: 0.0091 - mean_squared_error: 0.0091 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x297a4de80>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrmodel = getLinearRegressionModel(len(X.columns.values))\n",
    "lrmodel.fit(X,y, batch_size=1000000, epochs=30, validation_split=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-bfabc2cb93c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# xplot = list(range(len(y_test)))\n",
    "# plt.plot(xplot, lrpredictions)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(xplot, y_test)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2850542 samples, validate on 1221661 samples\n",
      "Epoch 1/10\n",
      "2850542/2850542 [==============================] - 8s 3us/step - loss: 0.0267 - acc: 0.0000e+00 - val_loss: 0.0241 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "2850542/2850542 [==============================] - 8s 3us/step - loss: 0.0251 - acc: 0.0000e+00 - val_loss: 0.0225 - val_acc: 0.0000e+00\n",
      "Epoch 3/10\n",
      "2850542/2850542 [==============================] - 7s 3us/step - loss: 0.0243 - acc: 0.0000e+00 - val_loss: 0.0224 - val_acc: 0.0000e+00\n",
      "Epoch 4/10\n",
      "2850542/2850542 [==============================] - 7s 3us/step - loss: 0.0241 - acc: 0.0000e+00 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 5/10\n",
      "2850542/2850542 [==============================] - 7s 3us/step - loss: 0.0240 - acc: 0.0000e+00 - val_loss: 0.0221 - val_acc: 0.0000e+00\n",
      "Epoch 6/10\n",
      "2850542/2850542 [==============================] - 7s 3us/step - loss: 0.0239 - acc: 0.0000e+00 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 7/10\n",
      "2850542/2850542 [==============================] - 7s 3us/step - loss: 0.0239 - acc: 0.0000e+00 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 8/10\n",
      "2850542/2850542 [==============================] - 7s 3us/step - loss: 0.0238 - acc: 0.0000e+00 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 9/10\n",
      "2850542/2850542 [==============================] - 7s 3us/step - loss: 0.0238 - acc: 0.0000e+00 - val_loss: 0.0220 - val_acc: 0.0000e+00\n",
      "Epoch 10/10\n",
      "2850542/2850542 [==============================] - 7s 3us/step - loss: 0.0238 - acc: 0.0000e+00 - val_loss: 0.0219 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x125ea1c88>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newnnmodel = getNNModel(3,15)\n",
    "hist = keras.callbacks.History()\n",
    "num_epochs = 10\n",
    "newnnmodel.fit(X, y, epochs=num_epochs, batch_size=1000000, callbacks=[hist], validation_split=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
